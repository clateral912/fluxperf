# Benchmark Configuration Example File
# Usage: python fluxperf.py --recipe recipe_example.yaml

# Global configuration
global:
  dataset: "datasets/sharegpt_clean.jsonl"
  endpoint: "http://127.0.0.1:8001/v1/chat/completions"
  model: "gpt-3.5-turbo"
  timeout: 300
  output_dir: "benchmark_results"
  
  # Mode selection: "dual_round" or "multi_turn"
  # dual_round: Two-round mode, suitable for datasets like LongBench
  # multi_turn: Multi-turn conversation mode, suitable for datasets like ShareGPT
  mode: "multi_turn"
  
  # Optional configuration
  max_input_length: null
  max_output_tokens: null
  max_context_tokens: null
  api_key: null
  shuffle_round2: true
  save_requests: false
  debug: false
  debug_log_dir: null
  
  # SLO configuration
  slo_file: null
  
  # Prometheus configuration
  prometheus_url: null
  prometheus_metrics: []
  
  # KV Cache configuration
  reset_cache_url: null
  reset_cache_between_rounds: false
  reset_cache_between_concurrency: false

# Mock Server Configuration (Optional)
mock_server:
  enabled: false
  host: "127.0.0.1"
  port: 8765

# vLLM Auto-Restart Configuration (Optional)
vllm:
  # Enable auto-restart: if true, vLLM will be restarted before each stage
  auto_restart: false
  
  # vLLM startup command (required if auto_restart is true)
  # Example: "vllm serve meta-llama/Llama-2-7b-chat-hf --port 8000"
  startup_command: null
  
  # Port for health check (default: 8000)
  port: 8000

# Test Stage Configuration
stages:
  - name: "Stage 1: Low Concurrency"
    # Environment variable settings for each stage
    env:
      VLLM_ATTENTION_BACKEND: "FLASH_ATTN"
      CUDA_VISIBLE_DEVICES: "0"
    
    # Concurrency level list
    concurrency_levels: [2, 4]
    
    # Number of samples for each concurrency level
    # If count matches concurrency_levels, they correspond one-to-one
    # Otherwise, use default rule (2*concurrency)
    num_samples: [4, 8]
  
  - name: "Stage 2: Medium Concurrency"
    env:
      VLLM_ATTENTION_BACKEND: "XFORMERS"
      CUDA_VISIBLE_DEVICES: "0,1"
    
    concurrency_levels: [8, 16]
    num_samples: [16, 32]
  
  - name: "Stage 3: High Concurrency"
    env:
      VLLM_ATTENTION_BACKEND: "FLASH_ATTN"
      CUDA_VISIBLE_DEVICES: "0,1,2,3"
    
    concurrency_levels: [32, 64]
    num_samples: [64, 128]
