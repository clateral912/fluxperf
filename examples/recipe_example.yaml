# 压测配置示例文件
# 使用方法: python dual_round_benchmarker.py --recipe recipe_example.yaml

# 全局配置
global:
  dataset: "datasets/sharegpt_clean.jsonl"
  endpoint: "http://127.0.0.1:8001/v1/chat/completions"
  model: "gpt-3.5-turbo"
  timeout: 300
  output_dir: "benchmark_results"
  
  # 模式选择: "dual_round" 或 "multi_turn"
  # dual_round: 两轮模式，适用于 LongBench 等数据集
  # multi_turn: 多轮对话模式，适用于 ShareGPT 等数据集
  mode: "multi_turn"
  
  # 可选配置
  max_input_length: null
  max_output_tokens: null
  max_context_tokens: null
  api_key: null
  shuffle_round2: true
  save_requests: false
  debug: false
  debug_log_dir: null
  
  # SLO 配置
  slo_file: null
  
  # Prometheus 配置
  prometheus_url: null
  prometheus_metrics: []
  
  # KV Cache 配置
  reset_cache_url: null
  reset_cache_between_rounds: false
  reset_cache_between_concurrency: false

# Mock Server 配置（可选）
mock_server:
  enabled: false
  host: "127.0.0.1"
  port: 8765

# 测试阶段配置
stages:
  - name: "Stage 1: Low Concurrency"
    # 每个阶段的环境变量设置
    env:
      VLLM_ATTENTION_BACKEND: "FLASH_ATTN"
      CUDA_VISIBLE_DEVICES: "0"
    
    # 并发层级列表
    concurrency_levels: [2, 4]
    
    # 每个并发层级的样本数
    # 如果数量与 concurrency_levels 匹配，则一一对应
    # 否则使用默认规则 (2*concurrency)
    num_samples: [4, 8]
  
  - name: "Stage 2: Medium Concurrency"
    env:
      VLLM_ATTENTION_BACKEND: "XFORMERS"
      CUDA_VISIBLE_DEVICES: "0,1"
    
    concurrency_levels: [8, 16]
    num_samples: [16, 32]
  
  - name: "Stage 3: High Concurrency"
    env:
      VLLM_ATTENTION_BACKEND: "FLASH_ATTN"
      CUDA_VISIBLE_DEVICES: "0,1,2,3"
    
    concurrency_levels: [32, 64]
    num_samples: [64, 128]
