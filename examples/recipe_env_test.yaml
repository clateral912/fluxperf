# Recipe for testing environment variable management functionality
# This recipe sets different environment variables in different stages and validates them during execution

global:
  dataset: "datasets/sharegpt_clean.jsonl"
  mode: "multi_turn"
  model: "test-model"
  timeout: 60
  output_dir: "env_test_results"

mock_server:
  enabled: true
  host: "127.0.0.1"
  port: 8765

# vLLM Auto-Restart Configuration (Optional)
vllm:
  # Enable auto-restart: if true, vLLM will be restarted before each stage
  auto_restart: false
  
  # vLLM startup command (required if auto_restart is true)
  # Example: "vllm serve meta-llama/Llama-2-7b-chat-hf --port 8000"
  startup_command: null
  
  # Port for health check (default: 8000)
  port: 8000

stages:
  - name: "Test Stage 1 - GPU 0"
    env:
      CUDA_VISIBLE_DEVICES: "0"
      TEST_STAGE_NAME: "stage_1"
      VLLM_ATTENTION_BACKEND: "FLASH_ATTN"
      MY_CUSTOM_VAR: "value_from_stage_1"
    concurrency_levels: [2]
    num_samples: [4]
  
  - name: "Test Stage 2 - GPU 0,1"
    env:
      CUDA_VISIBLE_DEVICES: "0,1"
      TEST_STAGE_NAME: "stage_2"
      VLLM_ATTENTION_BACKEND: "XFORMERS"
      MY_CUSTOM_VAR: "value_from_stage_2"
    concurrency_levels: [2]
    num_samples: [4]
  
  - name: "Test Stage 3 - All GPUs"
    env:
      CUDA_VISIBLE_DEVICES: "0,1,2,3"
      TEST_STAGE_NAME: "stage_3"
      VLLM_ATTENTION_BACKEND: "FLASH_ATTN"
      MY_CUSTOM_VAR: "value_from_stage_3"
      NEW_VAR_IN_STAGE_3: "only_in_stage_3"
    concurrency_levels: [2]
    num_samples: [4]
