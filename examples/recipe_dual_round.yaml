# Dual Round Mode Configuration Example
# Suitable for single-turn Q&A datasets like LongBench

global:
  dataset: "data/longbench_sample.jsonl"
  endpoint: "http://127.0.0.1:8001/v1/chat/completions"
  model: "gpt-3.5-turbo"
  mode: "dual_round"  # Key: Set to dual_round mode
  timeout: 300
  shuffle_round2: true
  output_dir: "benchmark_results_dual"

mock_server:
  enabled: true
  host: "127.0.0.1"
  port: 8765

# vLLM Auto-Restart Configuration (Optional)
vllm:
  # Enable auto-restart: if true, vLLM will be restarted before each stage
  auto_restart: false
  
  # vLLM startup command (required if auto_restart is true)
  # Example: "vllm serve meta-llama/Llama-2-7b-chat-hf --port 8000"
  startup_command: null
  
  # Port for health check (default: 8000)
  port: 8000

stages:
  - name: "Warm-up"
    env:
      TEST_PHASE: "warmup"
    concurrency_levels: [2]
    num_samples: [4]
  
  - name: "Low Load"
    env:
      TEST_PHASE: "low_load"
      CUDA_VISIBLE_DEVICES: "0"
    concurrency_levels: [4, 8]
    num_samples: [8, 16]
  
  - name: "High Load"
    env:
      TEST_PHASE: "high_load"
      CUDA_VISIBLE_DEVICES: "0,1"
    concurrency_levels: [16, 32]
    num_samples: [32, 64]
